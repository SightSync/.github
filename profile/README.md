# SightSync ğŸ‘€ğŸ—£ğŸ§ 

Welcome to the SightSync repository - a platform dedicated to assisting visually impaired individuals by delivering natural, accurate verbal descriptions of their surroundings. This project was developed with lots of â¤ï¸ by Oriol and Ferran for LauzHack2023 ğŸ†

## ğŸ“· What is SightSync?

SightSync is an application that uses open-source models ğŸ’¡ to assist visually-impaired individuals navigate their environment with greater ease and autonomy. It's essentially their â€˜eyesâ€™ with their voice description capabilities, feeding visual data through auditory channels. 

## ğŸ›  How do we do that?

We've built SightSync using the following state-of-the-art open-source models:

1. Zephyr: For understanding live environment scene structure at scale. The Linguistic-Landmarks Model (LLM) allows us to make sense of spatial data, an essential feature for our cause ğŸ” 

2. Whisper: To convert spoken language into text format (STT) ğŸ¤

3. FastPitch: For converting text description into voice(TTS) ğŸ§

4. GroundingDino: To provide item location details in a scenario, adding another layer of detail to our descriptive capabilities ğŸ“

5. CogVLM: To generate accurate context-aware description of surroundings, allowing us to craft immersive auditory experiences that accurately reflect an individual's environment ğŸŒ

Please note, all models are hosted on-prem ğŸ­

## ğŸ’ Contribute

As an open-source project, we welcome anyone who would like to contribute. We believe that every contribution, no matter how small, can make a big difference!

##  ğŸ“¬ Contact 

If you have a feature request, bug report, or just want to chat, don't hesitate to get in touch with us:

Oriol Agost - oriol@agost.info
Ferran Aran - ferran@aran.email

Thank you for your interest in SightSync. We're excited to see where we can go together! ğŸŒŸ 

